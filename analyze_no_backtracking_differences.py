"""
分析当 backtracking 次数为 0 时，D-APD 和 D-APDB 的差异
"""

print("="*80)
print("当 backtracking = 0 时，D-APD vs D-APDB 的差异分析")
print("="*80)

print("\n关键发现：即使 backtracking = 0 (eta_k = 1)，两个算法仍有差异！")
print("\n原因分析：")
print("\n1. **更新顺序的差异**")
print("   D-APD 的更新顺序：")
print("     Step 1: s[i] = s[i] + gamma_k * ((1 + eta_k) * x[i] - eta_k * x_prev[i])")
print("     Step 2: p_i_k = q[i] + eta_k * (q[i] - q_prev[i])")
print("     Step 3: x_next = prox_r_i(x[i] - tau_list[i] * (grad_phi_i(i, x[i]) + p_i_k), tau_list[i])")
print("     Step 4: theta_next = proj_dual(theta[i] + sigma_list[i] * g_i_of(i, x_next), B_i_list[i])")
print("     Step 5: q[i] = jacT_theta_i(i, x_next, theta_next) + Σ(s[i] - s[j])")
print("")
print("   D-APDB 的更新顺序（当 eta_k = 1 时）：")
print("     Backtracking 循环（即使不 backtracking 也会执行一次）：")
print("       - tau_tilde_i = tau_prev[i]")
print("       - eta_i_k = tau_prev[i] / tau_tilde_i = 1")
print("       - p_tilde_i = q[i] + eta_i_k * (q[i] - q_prev[i])")
print("       - x_tilde_kp1_i = prox_r_i(x[i] - tau_tilde_i * (grad_phi_i(i, x[i]) + p_tilde_i), tau_tilde_i)")
print("       - theta_tilde_kp1_i = proj_dual(theta[i] + sigma_tilde_i * g_i_of(i, x_tilde_kp1_i), B_i_list[i])")
print("     Step 1: tau_list[i] = tau_prev[i] / eta_k = tau_prev[i]")
print("     Step 2: s[i] = s[i] + gamma_k * ((1 + eta_k) * x[i] - eta_k * x_prev[i])")
print("     Step 3: p_i_k = q[i] + eta_k * (q[i] - q_prev[i])")
print("     Step 4: x[i] = x_tilde_kp1[i]  (使用 backtracking 循环中计算的值)")
print("     Step 5: theta[i] = theta_tilde_kp1[i]  (使用 backtracking 循环中计算的值)")
print("     Step 6: q[i] = jacT_theta_i(i, x[i], theta[i]) + Σ(s[i] - s[j])")

print("\n2. **关键差异：s[i] 的更新时机**")
print("   D-APD:")
print("     - 先更新 s[i]")
print("     - 然后计算 p_i_k（使用旧的 s[i]，但 p_i_k 不依赖 s[i]）")
print("     - 然后计算 x_next 和 theta_next")
print("     - 最后更新 q[i]（使用新的 s[i]）")
print("")
print("   D-APDB:")
print("     - 在 backtracking 循环中计算 x_tilde 和 theta_tilde（使用旧的 s[i]）")
print("     - 然后更新 tau_list[i]")
print("     - 然后更新 s[i]")
print("     - 然后计算 p_i_k（使用旧的 s[i]，但 p_i_k 不依赖 s[i]）")
print("     - 使用 backtracking 循环中计算的 x_tilde 和 theta_tilde")
print("     - 最后更新 q[i]（使用新的 s[i]）")
print("")
print("   关键点：D-APDB 在 backtracking 循环中计算 x_tilde 时，使用的是旧的 s[i]")
print("   而 D-APD 在计算 x_next 时，使用的是新的 s[i]（虽然 x_next 的计算不直接依赖 s[i]）")
print("   但是！q[i] 的更新依赖 s[i]，而 q[i] 会影响下一轮的 p_i_k")

print("\n3. **q[i] 更新的差异**")
print("   D-APD:")
print("     q[i] = jacT_theta_i(i, x_next, theta_next) + Σ(s[i] - s[j])")
print("     其中 s[i] 是已经更新过的（新的 s[i]）")
print("")
print("   D-APDB:")
print("     q[i] = jacT_theta_i(i, x[i], theta[i]) + Σ(s[i] - s[j])")
print("     其中 x[i] = x_tilde_kp1[i]（在 backtracking 循环中计算，使用旧的 s[i]）")
print("     但是 s[i] 是已经更新过的（新的 s[i]）")
print("")
print("   虽然 x_tilde 的计算不直接依赖 s[i]，但 q[i] 的更新依赖 s[i]")
print("   这会导致 q[i] 的值不同，进而影响下一轮的 p_i_k")

print("\n4. **gamma_k 计算的差异**")
print("   两个算法都使用相同的公式：")
print("     gamma_k = 1 / ((Σ_{i∈N} τ_max^0 d_i) * (2/c_α + η^k/c_ς))")
print("   当 eta_k = 1 时，两个算法的 gamma_k 应该相同")
print("   但是！D-APDB 在计算 gamma_k 之前，已经更新了 tau_list[i]")
print("   虽然当 eta_k = 1 时，tau_list[i] = tau_prev[i]，但更新顺序不同")

print("\n5. **数值误差累积**")
print("   即使数学上等价，由于：")
print("   - 计算顺序不同")
print("   - 浮点数运算的舍入误差")
print("   - 这些误差会随着迭代累积")
print("   导致两个算法的结果逐渐偏离")

print("\n6. **最关键的差异：E_i^k 的计算**")
print("   即使 backtracking 条件满足（不 backtracking），D-APDB 仍然会：")
print("   - 计算 E_i^k 的值（用于检查 backtracking 条件）")
print("   - E_i^k 的计算涉及复杂的公式，包括 Term 5")
print("   - Term 5 的计算方式取决于 E_use_gradient_form 参数")
print("   - 在 main-qcqp.py 中，E_use_gradient_form=True")
print("   - 这意味着 Term 5 使用梯度形式：2<∇φ_i(x) - ∇φ_i(x_i^k), x - x_i^k>")
print("   - 而 D-APD 根本不计算 E_i^k")
print("")
print("   虽然 E_i^k 的值不影响最终结果（因为不 backtracking），但计算过程本身")
print("   可能会因为浮点数运算顺序导致微小的数值差异")

print("\n结论：")
print("即使 backtracking = 0，两个算法仍然会有差异，因为：")
print("1. 更新顺序不同（s[i] 的更新时机）")
print("2. q[i] 的更新依赖 s[i]，而 s[i] 的更新时机不同")
print("3. 数值误差累积")
print("4. E_i^k 的计算（即使不影响结果，但计算过程可能引入误差）")
print("")
print("要完全消除差异，需要确保两个算法的更新顺序完全一致。")

print("\n" + "="*80)

